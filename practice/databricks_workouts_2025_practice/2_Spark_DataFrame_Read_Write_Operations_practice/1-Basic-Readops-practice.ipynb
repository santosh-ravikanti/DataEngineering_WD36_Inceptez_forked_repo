{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4fad378f-70c9-41b6-9f64-1ca4e9f56a7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#### Basic Catalog + Volume Feature of Databricks (We are learning how to build & use Databricks DATALAKE)\n",
    "Filesystem Hierarchy of Volume in Databricks (DBFS)?<br>\n",
    "Catalog -> /OurWorkspace/catalog/schema(database)/volume/folder/data files<br>\n",
    "Tables Hierarchy of Databricks?<br>\n",
    "Catalog -> /OurWorkspace/catalog/schema(database)/tables/data(dbfs filesystem/some other filesystems)<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7a792120-045d-4a8e-be46-186a83ab50a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "create catalog if not exists wd36_catalog;\n",
    "create database if not exists wd36_catalog.wd36_schema;\n",
    "create volume if not exists wd36_catalog.wd36_schema.wd36_volume;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a165bfbc-4536-406f-acef-c3dfc34f5e8a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "dbutils.fs.mkdirs(\"/Volumes/wd36_catalog/wd36_schema/wd36_volume/wd36_directory\")\n",
    "#Upload the drive data into this location..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ddd4e1e1-467d-421d-86c6-ab5c26c4a72a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "Download the data from the below drive url <br>\n",
    "https://drive.google.com/drive/folders/1Tw7V9eBtUxy0xQMW38z3-bzWI_ewzLm6?usp=drive_link"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba0588c8-0035-40ac-aa91-17479e4199af",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "We need to create spark session object by instantiating sparksession class (by default databricks did that if you create a notebook)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "381a6f5a-f856-4afb-ab1c-a1cb49d5dee8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.session import SparkSession\n",
    "print(spark)#already instantiated by databricks\n",
    "spark1=SparkSession.builder.getOrCreate()\n",
    "print(spark1)#we instantiated"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "220a856e-eeb5-4afd-8355-a1b4bdb1c27d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Create a DBFS volume namely commondata and upload the above data in that volume\n",
    "What are other FS uri's available? file:///, hdfs:///, dbfs:///, gs:///, s3:///, adls:///, blob:///"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "da503d1c-553a-4eac-9b3c-6e57527269d8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####How to Read/Extract the data from the filesytem and load it into the distributed memory for further processing/load - using diffent methodologies/options from different sources(fs & db) and different builtin formats (csv/json/orc/parquet/delta/tables)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "d87bdfec-949a-4bff-9d14-ebfdf897aac8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "csv_df1=spark.read.csv(\"dbfs:////Volumes/wd36_catalog/wd36_schema/wd36_volume/wd36_directory/custs.txt\")\n",
    "#csv_df1.show(2)#display with produce output in a dataframe format\n",
    "print(csv_df1.printSchema())\n",
    "display(csv_df1)#display with produce output in a beautified table format, specific to databricks\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "975feb5e-4bc6-423c-9ad2-b802de601fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Sample data of custs_1\n",
    "4000001,Kristina,Chung,55,Pilot<br>\n",
    "4000002,Paige,Chen,77,Teacher\n",
    "\n",
    "Sample data of custs_header<br>\n",
    "custid,fname,lname,age,profession<br>\n",
    "4000001,Kristina,Chung,55,Pilot<br>\n",
    "4000002,Paige,Chen,77,Teacher"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b078d4b3-6a02-4d01-ba53-e1b62c5dc540",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df2=spark.read.csv(\"/Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header.txt\",header=True)\n",
    "print(csv_df1.printSchema()) #display with produce output in a dataframe format\n",
    "csv_df2.show(2) #display with produce output in a dataframe format\n",
    "display(csv_df2.limit(2)) #display with produce output in a beautified table format, specific to databricks limiting only top 2 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "163395db-6b1b-4927-a9a8-47a5e4d8bc01",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#1. Practicing Header, inferSchema and toDF (to exclusively mention column names)\n",
    "csv_df3=spark.read.csv(\"/Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header.txt\",header=True, inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df3.printSchema()) #display with produce output in a dataframe format\n",
    "csv_df3.show(2) #display with produce output in a dataframe format\n",
    "display(csv_df3.limit(2)) #display with produce output in a beautified table format, specific to databricks limiting only top 2 records\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "009a120e-864d-4a4d-9117-147d3664dbe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#2. Printing Schema (equivalent to describe table)\n",
    "csv_df1.printSchema()\n",
    "csv_df2.printSchema()\n",
    "csv_df3.printSchema()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e53ae0a5-c97f-4b89-a77a-1fbf22ff7faf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Performance Importance: Though inferSchema has to be used causiously, we can improve performance by using an option to reduce the data scanned for large data.."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5ee3059d-3a82-46ba-adaa-2cfeaf3753df",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#3. As inferschema scans whole data using sampling ratio for performance improvement\n",
    "csv_df4=spark.read.csv(\"/Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header.txt\",header=True, inferSchema=True,samplingRatio=0.10 ).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "csv_df4.where(\"id in (4004979,4004981)\").show(2)\n",
    "csv_df4.printSchema()\n",
    "display(csv_df4.limit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "8a196167-00e2-4fdc-bbdd-89038a897939",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#4. Using delimiter or seperator option - /Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs - Copy.csv\n",
    "csv_df5=spark.read.csv(\"/Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs - Copy.csv\",header=False, inferSchema=True, sep='~')\n",
    "#csv_df5.where(\"id in (4004979,4004981)\").show(2)\n",
    "csv_df5.printSchema()\n",
    "display(csv_df5.limit(2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f462c9a4-cfb3-4486-80f3-39455e717851",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#5. Using different options to create dataframe with csv and other module... (2 methodologies (spark.read.inbuiltfunction or spark.read.format(anyfunction).load(\"path\")) with 3 ways of creating dataframes (pass parameters to the csv()/option/options))\n",
    "csv_df6=spark.read.csv(\"dbfs:///Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header.txt\",inferSchema=True,header=True,sep='~')\n",
    "csv_df6.show(2)\n",
    "#or another way of creating dataframe (from any sources whether builtin or external)...\n",
    "#option can be used for 1 or 2 option...\n",
    "csv_df7=spark.read.option(\"header\",\"True\").option(\"inferSchema\",\"true\").option(\"sep\",\"~\").format(\"csv\").load(\"dbfs:///Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header.txt\")\n",
    "csv_df7.show(2)\n",
    "#options can be used for multiple options in one function as a parameter...\n",
    "csv_df8=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").format(\"csv\").load(\"dbfs:///Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header.txt\")\n",
    "csv_df8.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ab7357e4-dc93-46c3-afa3-b8d1268d855c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Generic way of read and load data into dataframe using fundamental options from built in sources (csv/orc/parquet/xml/json/table) (inferschema, header, sep)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3854a111-973f-44c3-a0de-eb21b8c7e4cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df9=spark.read.csv(\"dbfs:///Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header.txt\",inferSchema=True,header=True,sep='~')\n",
    "csv_df9.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "4b7f4a41-0b3d-4e84-9349-48305109355a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Generic way of read and load data into dataframe using extended options from external sources (bigquery/redshift/athena/synapse) (tmpfolder, access controls)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52edb76c-a8e0-44ec-889d-421b73cf1996",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#options can be used for multiple options in one function as a parameter...\n",
    "csv_df10=spark.read.options(header=\"True\",inferSchema=\"true\",sep=\"~\").format(\"csv\").load(\"dbfs:///Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header.txt\")\n",
    "csv_df10.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "058e11df-a6de-4acf-8fc4-162b237b0f0a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##### Reading data from multiple files & Multiple Path (We still have few more options)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "72c96c04-b25a-40af-83ac-2b9e1aaed613",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "csv_df11=spark.read.csv(path=[\"dbfs:///Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs.txt\",\"dbfs:///Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs*\"],inferSchema=True,header=True,sep=',')\n",
    "print(csv_df11.count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "982272bb-381a-4fff-9f9a-636b6a9a74f1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####Requirement: I am getting data from different source systems of different regions (NY, TX, CA) into different landing pad (locations), how to access this data?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bc5e8c61-eec9-4d7a-b560-18b81004af39",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "df_multiple_sources=spark.read.csv(path=[\"/Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header - NY.csv\",\"/Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header - TX.csv\"],inferSchema=True,header=True,sep=',',pathGlobFilter=\"custs_header*\",recursiveFileLookup=True)\n",
    "#.toDF(\"cid\",\"fn\",\"ln\",\"a\",\"p\")\n",
    "print(df_multiple_sources.count())\n",
    "df_multiple_sources.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "eccae843-0a73-4526-ae12-2349f4a64533",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "### Provide schema with SQL String or programatically (very very important)\n",
    "[PySpark SQL Datatypes](https://spark.apache.org/docs/latest/sql-ref-datatypes.html) <br>\n",
    "[Data Types](https://spark.apache.org/docs/latest/api/python/reference/pyspark.sql/data_types.html)<br>\n",
    "###To provide schema (columname & datatype), what are the 2 basic options available that we learned so far ? inferSchema/toDF<br>\n",
    "###We are going to learn additionally 2 more options to handle schema (colname & datatype)?<br>\n",
    "###1. Using simple string format of define schema.<br>\n",
    "###IMPORTANT: 2. Using structure type to define schema.<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f74d9fa-cf4b-473c-9a9c-0e623856c9d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#By default it will use _c0,_c1..._cn it will apply as column headers, if we use toDF(colnames) we can define our own headers..\n",
    "\n",
    "csv_df12=spark.read.csv(\"/Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header1.csv\").toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df12.printSchema())\n",
    "csv_df12=spark.read.csv(\"/Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header1.csv\",header=True,inferSchema=True).toDF(\"id\",\"fname\",\"lname\",\"age\",\"prof\")\n",
    "print(csv_df12.printSchema())\n",
    "\n",
    "#1. Using simple string format of define custom simple schema.\n",
    "str_struct=\"id integer,fname string,lname string,age integer,prof string\"\n",
    "csv_df13=spark.read.schema(str_struct).options(header=\"true\").csv(\"/Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header1.csv\")\n",
    "print(csv_df13.printSchema())\n",
    "csv_df13.show(2)\n",
    "\n",
    "#2. Important part - Using structure type to define custom complex schema.\n",
    "#4000001,Kristina,Chung,55,Pilot\n",
    "#pattern - \n",
    "#import the types library based classes..\n",
    "# define_structure=StructType([StructField(\"colname\",DataType(),True),StructField(\"colname\",DataType(),True)...])\n",
    "from pyspark.sql.types import StructType,StructField,StringType,IntegerType\n",
    "custom_schema=StructType([StructField(\"id\",IntegerType(),False),StructField(\"fname\",StringType(),True),StructField(\"lname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"prof\",StringType())])\n",
    "csv_df14=spark.read.schema(custom_schema).options(header=True).csv(\"/Volumes/wd36_catalog/wd36_schema/ingestion_volume/source_files/custs_header1.csv\")\n",
    "print(csv_df14.printSchema())\n",
    "csv_df14.show(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "5ca48da7-bbbb-4710-aa50-f648e7c7b7cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "what will happen if one of the file have an extra column, rest all the columns are same?<br>\n",
    "Is union operation performing here in dataframe? not directly, but we can do to answer above question.. unionByName (Schema evolution)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d39d924f-87e5-4881-a233-dae388a368c2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "738627ae-187f-4a94-900c-a3c66a2bbcdc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What are all the overall options we used in this notebook, for learning fundamental spark csv read operations?\n",
    "1. How to create/manage Catalog & Volume\n",
    "2. spark session\n",
    "3. How to create some sample data and push it to the volume/folder/file\n",
    "4. spark.read.csv operations & spark.read.format().load()\n",
    "5. Few of the important read options under csv such as header, sep, inferSchema, toDF.\n",
    "6. How to define custom schema/structure using mere string with colname & datatype or by using StructType([StructField(colname,DataType()...)]) (very important).\n",
    "7. Few additional options such as samplingRatio, recursiveFileLookup & pathGlobFilter for accessing folders and subfolders with some pattern of filenames\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d62999dc-9df6-49b4-a1dc-c685478011ae",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "What are all the overall options we used in this notebook, for learning fundamental spark dataframe write operations in different formats and targets?\n",
    "1. df.write.csv/json/orc/parquet/table/xml... operations & df.write.format('delta').save()\n",
    "2. Few of the important read options under csv such as header, sep, mode(append/overwrite/error/ignore), toDF.\n",
    "3. Few additional options such as compression, different file formats..."
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": {
    "hardware": {
     "accelerator": null,
     "gpuPoolId": null,
     "memory": "STANDARD"
    }
   },
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "3"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 1477795238740280,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "1-Basic-Readops-practice",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
