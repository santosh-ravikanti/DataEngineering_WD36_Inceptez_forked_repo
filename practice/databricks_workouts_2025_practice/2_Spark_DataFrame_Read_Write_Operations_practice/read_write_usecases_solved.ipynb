{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "6d3074a4-d6d0-4e14-9806-280f7874ab93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "\n",
    "#### Telecom Domain Read & Write Ops Assignment ‚Äì Building Datalake & Lakehouse\n",
    "\n",
    "###### First Import all required libraries & Create spark session object"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9e084742-ce99-4495-ba90-9e9960e5c409",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.sql.types import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "cdc336a5-682a-4611-89f2-5818769fa684",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### create Sample Telecom Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "664b926e-3b61-4821-ab52-dfd454c5e100",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "telecom_data = [\n",
    "    (1, \"Airtel\", \"Prepaid\", 249.0, \"Active\"),\n",
    "    (2, \"Jio\", \"Postpaid\", 399.0, \"Active\"),\n",
    "    (3, \"Vi\", \"Prepaid\", 199.0, \"Inactive\"),\n",
    "    (4, \"BSNL\", \"Prepaid\", 149.0, \"Active\")\n",
    "]\n",
    "\n",
    "schema = [\"customer_id\", \"operator\", \"plan_type\", \"monthly_charge\", \"status\"]\n",
    "\n",
    "df = spark.createDataFrame(telecom_data, schema)\n",
    "df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "15aafd52-da6e-40eb-a76f-213fb8811446",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Create Catalog, Schema and Volume"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "implicitDf": true,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4439dd52-72ee-4e72-9f53-188a16e32e9f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%sql\n",
    "CREATE CATALOG IF NOT EXISTS telecom_catalog_assign;\n",
    "CREATE SCHEMA IF NOT EXISTS telecom_catalog_assign.landing_zone;\n",
    "CREATE VOLUME IF NOT EXISTS telecom_catalog_assign.landing_zone.landing_vol;"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "d5eb9ac2-5ab7-4e48-90b3-7e6356c8fdb4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### Using dbutils.fs.mkdirs, create folders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b81f220-bfaa-4544-a199-6a281f87ee93",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Create Customer folder\n",
    "dbutils.fs.mkdirs(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/\"\n",
    ")\n",
    "\n",
    "# Create usage folder\n",
    "dbutils.fs.mkdirs(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/\"\n",
    ")\n",
    "\n",
    "# Create tower folder\n",
    "dbutils.fs.mkdirs(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\"\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "07a3e52a-cef1-46d3-bec2-2b35fbb7c617",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Difference between Volume vs DBFS/ Filestore\n",
    "\n",
    "DBFS/FileStore ‚Üí Dev, temporary, non-governed storage\n",
    "\n",
    "Volumes ‚Üí Secure, governed, production-ready storage for regulated data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b95ab364-40b5-4afc-9ab2-0cef7a3d7284",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##Data files to use in this usecase:\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "264ad50e-4887-467e-aa00-d602ccd81cd6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# step 1: Define Raw data files (as given)\n",
    "\n",
    "customer_csv = '''\n",
    "101,Arun,31,Chennai,PREPAID\n",
    "102,Meera,45,Bangalore,POSTPAID\n",
    "103,Irfan,29,Hyderabad,PREPAID\n",
    "104,Raj,52,Mumbai,POSTPAID\n",
    "105,,27,Delhi,PREPAID\n",
    "106,Sneha,abc,Pune,PREPAID\n",
    "'''\n",
    "\n",
    "usage_tsv = '''customer_id\\tvoice_mins\\tdata_mb\\tsms_count\n",
    "101\\t320\\t1500\\t20\n",
    "102\\t120\\t4000\\t5\n",
    "103\\t540\\t600\\t52\n",
    "104\\t45\\t200\\t2\n",
    "105\\t0\\t0\\t0\n",
    "'''\n",
    "\n",
    "tower_logs_region1 = '''event_id|customer_id|tower_id|signal_strength|timestamp\n",
    "5001|101|TWR01|-80|2025-01-10 10:21:54\n",
    "5004|104|TWR05|-75|2025-01-10 11:01:12\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "ba5942c2-6570-4220-97b3-79ffaa04b9c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##2. Filesystem operations\n",
    "1. Write dbutils.fs code to copy the above datasets into your created Volume folders:\n",
    "Customer ‚Üí /Volumes/.../customer/\n",
    "Usage ‚Üí /Volumes/.../usage/\n",
    "Tower (region-based) ‚Üí /Volumes/.../tower/region1/ and /Volumes/.../tower/region2/\n",
    "\n",
    "2. Write a command to validate whether files were successfully copied"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ed7e482-329a-4274-9cd0-b1d5e177f410",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 2: Write raw files into landing volume folders\n",
    "# Customer CSV --> landing/customer\n",
    "dbutils.fs.put(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\",\n",
    "    customer_csv,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Usage TSV --> landing/usage\n",
    "dbutils.fs.put(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\",\n",
    "    usage_tsv,\n",
    "    overwrite=True\n",
    ")\n",
    "\n",
    "# Tower logs --> landing/tower\n",
    "dbutils.fs.put(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.csv\",\n",
    "    tower_logs_region1,\n",
    "    overwrite=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26454705-332d-42bf-af75-a36b952f4c7f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 3: Read customer CSV file\n",
    "customer_df = spark.read.csv(\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\"\n",
    ")\n",
    "customer_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "23fc93f0-62cf-446e-b661-dc6b802d3bb1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 4: Read customer csv with correct options\n",
    "\n",
    "customer_df = spark.read \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\") \\\n",
    "    .toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan_type\")\n",
    "\n",
    "customer_df.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6c021f47-74f4-46bd-90b6-a10d30092256",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 5: Read usage tsv file\n",
    "usage_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\") \n",
    "    #.toDF(\"customer_id\", \"voice_mins\", \"data_mb\", \"sms_count\")\n",
    "usage_df.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cad46afc-682b-4f60-bf8a-59733aa60b3c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Step 6: Read tower logs pipe delimited\n",
    "tower_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.csv\") \n",
    "    #.toDF(\"event_id\", \"customer_id\", \"tower_id\", \"signal_strength\", \"timestamp\")\n",
    "tower_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "76c0e195-55ee-4f77-ab78-bd4e8bf86755",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Spark Directory Read Use Cases\n",
    "1. Read all tower logs using:\n",
    "Path glob filter (example: *.csv)\n",
    "Multiple paths input\n",
    "Recursive lookup\n",
    "\n",
    "2. Demonstrate these 3 reads separately:\n",
    "Using pathGlobFilter\n",
    "Using list of paths in spark.read.csv([path1, path2])\n",
    "Using .option(\"recursiveFileLookup\",\"true\")\n",
    "\n",
    "3. Compare the outputs and understand when each should be used."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "056ed8ba-518f-423c-8b69-f685774d2633",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use case 1: Read using pathGlobalFilter\n",
    "\n",
    "Description <br>\n",
    "Reads files matching a specific filename pattern within a single directory\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ee8d6e2f-9228-45f4-a274-8296832a413f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Use case 1: Read using pathGlobalFilter\n",
    "\n",
    "Description <br>\n",
    "Reads files matching a specific filename pattern within a single directory\"\"\"\n",
    "\n",
    "tower_glob_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .option(\"pathGlobalFilter\", \"*.log\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "\n",
    "tower_glob_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "c7e35229-9b6c-4d81-8979-ca7246e2db72",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use case 1: When to use\n",
    "\n",
    "- Files are in one directory\n",
    "- Naming pattern is consistent\n",
    "- No subfolders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4c022d0e-b04d-46f0-b9cd-5cb3b959df30",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Use Case 2: Read using Multiple Paths Input\n",
    "Description <br>\n",
    "\n",
    "Explicitly specify exact file paths to read.\"\"\"\n",
    "\n",
    "paths = [\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region1.csv\",\n",
    "    \"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/tower_logs_region2.csv\"\n",
    "]\n",
    "\n",
    "tower_multi_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .csv(paths)\n",
    "\n",
    "tower_multi_df.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f0b835fd-b2f4-4cda-b4c3-e654883289c1",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use case 2: When to use\n",
    "\n",
    "- You know exact file names\n",
    "- Reading specific files only\n",
    "- Reprocessing selected data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "52de85bd-b68a-4ff8-aeee-a18239d2d8ef",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\"\"\"Use Case 3: Read using Recursive File Lookup\n",
    "Description <br>\n",
    "\n",
    "Reads files recursively from all subdirectories.\"\"\"\n",
    "\n",
    "tower_recursive_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"sep\", \"|\") \\\n",
    "    .option(\"recursiveFileLookup\", \"true\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/tower/\")\n",
    "\n",
    "tower_recursive_df.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "60c889e8-38ce-447f-8c0f-b146e9be6d46",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Use Case 3: When to use\n",
    "\n",
    "- Deep folder hierarchy\n",
    "- Partitioned data\n",
    "- Unknown or dynamic folder structure"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a2cba52b-5e0c-4d2a-b795-378ab4fe81d2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Final One-Line Summary (Exam Ready)\n",
    "\n",
    "- pathGlobFilter ‚Üí Pattern-based read in a single directory\n",
    "- Multiple paths ‚Üí Full control over files read\n",
    "- recursiveFileLookup ‚Üí Auto-discovery across subfolders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "055815dd-8fa8-4aad-8a3a-2f0e76af0b41",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##4. Schema Inference, Header, and Separator\n",
    "1. Try the Customer, Usage files with the option and options using read.csv and format function:<br>\n",
    "header=false, inferSchema=false<br>\n",
    "or<br>\n",
    "header=true, inferSchema=true<br>\n",
    "2. Write a note on What changed when we use header or inferSchema  with true/false?<br>\n",
    "3. How schema inference handled ‚Äúabc‚Äù in age?<br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7f2f3537-71f0-4278-bd21-acd80e75c6da",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Case 1: customer file --> header = false, inferschema = false\n",
    "cust_no_header_no_schema_df = spark.read \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\")\n",
    "\n",
    "cust_no_header_no_schema_df.show()\n",
    "cust_no_header_no_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "263b9732-a7a3-42f2-9f52-630c4c66b682",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Case 1: usage file --> header = false, inferschema = false\n",
    "usage_no_header_no_schema_df = spark.read \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"inferSchema\", \"false\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\")\n",
    "\n",
    "usage_no_header_no_schema_df.show()\n",
    "usage_no_header_no_schema_df.printSchema()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5553538-f4cf-43b7-a722-ec641a431159",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Case 2: Header = True and inferschema = True\n",
    "# customer file\n",
    "customer_header_schema_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\") \\\n",
    "    .toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan_type\")\n",
    "\n",
    "customer_header_schema_df.show()\n",
    "customer_header_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f9006848-b6d6-45de-bb46-df99fcbfc65d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "# Case 2: Header = True and inferschema = True\n",
    "# usage file\n",
    "usage_header_schema_df = spark.read \\\n",
    "    .option(\"header\", \"true\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .option(\"sep\", \"\\t\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/usage/usage.tsv\") \\\n",
    "    .toDF(\"customer_id\", \"voice_mins\", \"data_mb\", \"sms_count\")\n",
    "\n",
    "usage_header_schema_df.show()\n",
    "usage_header_schema_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3a62aa1a-2cb7-4e91-8808-3cda51e4b9c7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### 2. Write a note on What changed when we use header or inferSchema with true/false?\n",
    "üîπ header = false\n",
    "Spark treats first row as data\n",
    "Column names default to _c0, _c1, etc.\n",
    "Manual renaming required\n",
    "\n",
    "üîπ header = true\n",
    "First row becomes column names\n",
    "Data is more readable and usable\n",
    "Prevents accidental ingestion of header as data\n",
    "\n",
    "üîπ inferSchema = false\n",
    "All columns are read as string\n",
    "No type validation\n",
    "Faster read but unsafe for analytics\n",
    "\n",
    "üîπ inferSchema = true\n",
    "Spark scans data to determine data types\n",
    "Enables numeric operations and aggregations\n",
    "Slight performance overhead"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f5bfaa44-3678-4c70-8264-7d3dfd86e2a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###### 3.How schema inference handled ‚Äúabc‚Äù in age?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6795f887-d314-43d9-ac18-d5e63be52a9d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_header_schema_df.select(\"customer_id\", \"age\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "61839f71-a2e9-49aa-ac19-390dfa70a6a3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##5. Column Renaming Usecases\n",
    "\n",
    "##### 1. Apply column names using string using toDF function for customer data\n",
    "\n",
    "###### Use Case:\n",
    "###### File has no header and all columns are read as strings.\n",
    "###### We only want to rename columns not enforce data types yet."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "365e6b58-ed6d-44fb-8bff-0687edb84db8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "customer_df = spark.read \\\n",
    "    .option(\"header\", \"false\") \\\n",
    "    .option(\"inferSchema\", \"true\") \\\n",
    "    .csv(\"/Volumes/telecom_catalog_assign/landing_zone/landing_vol/customer/customer.csv\") \\\n",
    "    .toDF(\"customer_id\", \"name\", \"age\", \"city\", \"plan_type\")\n",
    "\n",
    "customer_df.show()\n",
    "customer_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "69c8cea1-bc32-4317-bb61-838245a96000",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######2. Apply column names and datatype using the schema function for usage data\n",
    "Use-case:\n",
    "File has a header, but we want full control of schema."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3834e25b-0cdb-4fc8-930b-b778f9ecc6a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8989119920251182,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "read_write_usecases_solved",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
