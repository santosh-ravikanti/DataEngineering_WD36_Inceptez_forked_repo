{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "26d7f53f-32c8-40ba-982d-04c7548932b9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#Very Important Spark Learning - BY LEARNING this PROGRAM - WE BECOME A DATA ENGINEER (DATA CURATION DEVELOPER & DATA ANALYST)\n",
    "Simply say- We are going to learn...\n",
    "next level of SQL (Spark SQL) + Python Function based programming (Framework of Spark DSL) + Datawarehouse (Datalake+Lakehouse) -> Transformation & Analytics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37a76be2-1041-4fc6-b94b-ab46b78e169c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**1. Data Munging** - (Cleanup) Process of transforming and mapping data from Raw form into Tidy(usable) format with the intent of making it more appropriate and valuable for a variety of downstream purposes such for further Transformation/Enrichment, Egress/Outbound, analytics, Datascience/AI application & Reporting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9435574f-1e19-4ddd-acc4-b5f11c5fcd5e",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "**Passive Data Munging** - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns. <br>\n",
    "\n",
    "**Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dc009f1a-5864-4b7b-8697-1dd72790eb62",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###a. Passive Data Munging - \n",
    "- Visible - Data Discovery/Data Exploration/ EDA (Exploratory Data Analytics) (every layers ingestion/transformation/analytics/consumption) - Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b76842c2-34fc-43a7-8138-e331de2af1cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Visibily/Manually opening the file we found couple of data patterns (Manual Exploratory Data Analysis)\n",
    "- It is a Structured data with comma seperator (CSV)\n",
    "- No Header, No comments, footer is there in the data\n",
    "- Total columns are (seperator + 1)\n",
    "- Data Quality \n",
    "- - Null columns & null rows are there\n",
    "- - duplicate rows & Duplicate keys\n",
    "- - format issues are there (age is not in number format eg. 7-7)\n",
    "- - Uniformity issues (Artist, artist)\n",
    "- - Number of columns are more or less than the expected\n",
    "- eg. 4000011,Francis,McNamara,47,Therapist,NewYork & 4000014,Beth,Woodard,65\n",
    "- - Identification of data type"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "88ccadc2-69ed-4744-8673-b83a29cd0eaa",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Programatically lets try to find couple of data patterns applying EDA - passively (without modifying, just for description).\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "1623cdd4-18ac-432b-a988-f99ab46e73e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We have to define Spark Session to enter into Spark application\n",
    "from pyspark.sql.session import SparkSession\n",
    "spark=SparkSession.builder.appName(\"we47_Bread_n_Butter2_Important_Application\").getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "38b7e978-9f46-4506-8ab2-c79f8bcf1089",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766808978576}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Data Exploration programatically\n",
    "rawdf1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")\n",
    "rawdf1.show(20,False)\n",
    "display(rawdf1.take(20))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "870e142d-523c-49a3-8c70-1cbd96d00377",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive Munging - EDA of schema/structure functions we can use\n",
    "rawdf1.printSchema()#I am realizing the id & age columns are having some non numerical values (supposed to be numeric)\n",
    "print(rawdf1.columns)#I am understanding the column numbers/order and the column names\n",
    "print(rawdf1.dtypes)#Realizing the datatype of every columns (even we can do programattic column & type identification for dynamic programming)\n",
    "print(rawdf1.schema)#To identify the structure of the data in the StructType and StructField format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b9ed1b9-96c1-4f58-939a-320911e29b9c",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Important passive Datamunging - EDA data functions we can use\n",
    "#We identified few patterns on this data\n",
    "#1. Deduplication of rows and given column(s)\n",
    "#2. Null values ratio across all columns\n",
    "#3. Distribution (Dense) of the data across all number columns\n",
    "#4. Min, Max values\n",
    "#5. StdDeviation - Standard deviation tells you how much the data varies from the average (mean).\n",
    "#6. Percentile - Distribution percentage from 0 to 100 in 4 quadrants of 25%\n",
    "print(\"actual count of the data\",rawdf1.count())\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.distinct().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated record (all columns) count\",rawdf1.dropDuplicates().count())#de duplicate the entire columns of the given  dataframe\n",
    "print(\"de-duplicated given cid column count\",rawdf1.dropDuplicates(['id']).count())#de duplicate the entire columns of the given  dataframe\n",
    "display(rawdf1.describe())\n",
    "display(rawdf1.summary())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2ccb7e0e-aeec-4789-aecd-61187cacd34b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "###**b. Active Data Munging**\n",
    "1. Combining Data + Schema Evolution/Merging (Structuring)\n",
    "2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)\n",
    "3. De Duplication and Levels of Standardization () of Data to make it in a usable format (Dataengineers/consumers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5a231c83-377c-455d-b0d4-665fc66bf9cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Questions related to multiple files/paths/sub path handling\n",
    "#I have data in different filenames in a single/multiple location, i need to read all these data in a df - path=[\"path1/file1\",\"path1/file2\",\"path2/file3\"]\n",
    "#I have data in single pattern of file names in a single/multiple locations or subfolders, i need to read all these data in a df - path=[\"path1/\",\"path1/\",\"path2/\"], pathGlobFilter=\"custsm*\", recursiveFileLookup=True\n",
    "\n",
    "#Questions related handling evolving data structure with data ingested in different days/periods - Ans. Schema Evolution\n",
    "#Evolution is growth over the time (Filesystem level).. Eg. Source is sending data with additional columns week over week in csv format\n",
    "#1. Read and write in Serialized format( ORC,Parquet)\n",
    "#2. Read DF with mergeSchema = True\n",
    "\n",
    "#Questions related handling data from different sources with different related structure in a same day - Ans. Schema Merging/Melting (Dataframe level).. Eg. Source1 is sending custsmodified_NY with 5 columns and Source2 is sending custsmodified TX with 4 columns\n",
    "#1. Read file1 in DF1, read file2 in DF2\n",
    "#2. Create DF3 by merging DF1 and DF2 using df1.unionByName(df2,allowMissingColumns=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "61c30b1f-5451-4ca3-855e-cb218b514988",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####1. Combining Data + Schema Evolution/Merging (Structuring) - Preliminary Datamunging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3dbd0c39-1220-45a5-96dd-8ac93d4990cd",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Extraction (Ingestion) methodologies\n",
    "#1. Single file\n",
    "struct1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(struct1).csv(path=\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\")\n",
    "#2. Multiple files (with different names)\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",\"/Volumes/we47catalog/we47schema/we47_volume/custsmodifiedsample.txt\"])\n",
    "#3. Multiple files in multiple paths or sub paths\n",
    "rawdf1=spark.read.schema(struct1).csv(path=[\"/Volumes/we47catalog/we47schema/we47_volume/\",\"/Volumes/we47catalog/we47schema/we47_volume/we47_dir1\"],recursiveFileLookup=True,pathGlobFilter=\"custsm*\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b9d3ddef-c256-474f-b8de-cefad4bdb5f6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#VERY VERY IMPORTANT FOR PROACTIVE DRIVING OF INTERVIEW\n",
    "#Active Data munging...\n",
    "#When you go for Schema Merging/Melting and Schema Evolution?\n",
    "#Schema Merging/Melting (unionByName,allowMissingColumns)- If we get multiple files\n",
    "#Schema Evolution (orc/parquet with mergeSchema) - If no. of columns are keeps added by the source system\n",
    "#when we know structure of the file already - schema merge/ schema not known earlier  - schema evolution\n",
    "\n",
    "#COMBINING OR SCHEMA MERGING or SCHEMA MELTING of Data from different sources(Important interview question also as like schema evolution...)\n",
    "#4. Multiple files with different structure in multiple paths or sub paths\n",
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "display(rawdf1)\n",
    "display(rawdf2)\n",
    "rawdf_merged=rawdf1.union(rawdf2)#Use union only if the dataframes are having same columns in the same order with same datatype..\n",
    "display(rawdf_merged)\n",
    "#Expected right approach to follow\n",
    "rawdf_merged=rawdf1.unionByName(rawdf2,allowMissingColumns=True)\n",
    "display(rawdf_merged)\n",
    "\n",
    "#Lets try to achieve using schema evolution(but not applicable for this data munging activity of different dataframes available at the same time, we just get the dataframes merged/unioned without writing into serialized format)\n",
    "strt1=\"id string, firstname string, lastname string, age string, profession string\"\n",
    "rawdf1=spark.read.schema(strt1).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_N*\")\n",
    "strt2=\"id string, firstname string, age string, profession string,city string\"\n",
    "rawdf2=spark.read.schema(strt2).csv(path=[\"/Volumes/workspace/wd36schema/ingestion_volume/source/\"],recursiveFileLookup=True,pathGlobFilter=\"custsmodified_T*\")\n",
    "rawdf1.write.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/targetorc1/\",mode='overwrite')#Useful for future over the time\n",
    "rawdf2.write.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/targetorc1/\",mode='append')\n",
    "rawdf_evolved=spark.read.orc(\"/Volumes/workspace/wd36schema/ingestion_volume/targetorc1/\",mergeSchema=True)\n",
    "display(rawdf_evolved)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fee2be30-7760-4f90-b0e3-90be3b38a04b",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Just for the simple learning of schema evolution & schema merging/melting<br>\n",
    "Schema merging/melting<br>\n",
    "1,rajeshwari day1(source1)<br>\n",
    "1,rajeshwari,30 day1(source2)<br>\n",
    "\n",
    "Schema evolution<br>\n",
    "1,rajeshwari day1<br>\n",
    "1,rajeshwari,30 day2<br>\n",
    "\n",
    "Output is same in both cases...<br>\n",
    "id,name,age<br>\n",
    "1,rajeshwari,null<br>\n",
    "1,null,30<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e04175e7-a081-4ed6-873c-95e6d32d67e2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####2. Validation, Cleansing, Scrubbing - Cleansing (removal of unwanted datasets), Scrubbing (convert raw to tidy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6f4c6278-9af0-413b-8265-f7fb2ac26590",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Validation - Exploration of data by actively applying cleansing & scrubbing\n",
    "Identifying and filling gaps & Cleaning data to remove outliers and inaccuracies\n",
    "Preprocessing, Preparation\n",
    "Cleansing (removal of unwanted datasets eg. na.drop),\n",
    "Scrubbing (convert of raw to tidy na.fill or na.replace)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "f5760eab-394a-4d16-afd4-a6d39e371cb8",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10cb63a5-a86b-47ff-b7ad-fdbe47980a50",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#Validation by doing cleansing\n",
    "strt1=\"id int, firstname string, lastname string, age int, profession string\"#applying schema strictly to validate and cleanse the data\n",
    "strt11=StructType([StructField(\"id\",IntegerType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"profession\",StringType(),True)])\n",
    "dfmethod1=spark.read.schema(strt11).csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"PERMISSIVE\",header=False)\n",
    "#method1 - permissive with all rows with respective nulls\n",
    "#We are making nulls where ever data format mismatch is there (cutting down mud portition from potato - scrubbing)\n",
    "display(dfmethod1)\n",
    "print(\"entire count of data\",dfmethod1.count())\n",
    "print(\"after scrubbing, count of data\",len(dfmethod1.collect()))\n",
    "#or\n",
    "\n",
    "#method2 - drop malformed rows\n",
    "strt1=\"id int, firstname string, lastname string, age int, profession string\"#applying schema strictly to validate and \n",
    "dfmethod2=spark.read.schema(strt11).csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"dropMalformed\",header=False)\n",
    "#We are removing the entire row, where ever data format mismatch is there (number of columns, data type mismatch) (throwing away the entire potato or some portion of it by - cleansing)\n",
    "display(dfmethod2)\n",
    "print(\"entire count of data\",dfmethod2.count())\n",
    "print(\"after cleansing, count of data\",len(dfmethod2.collect()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "620fdbbd-f94f-4ff2-8f26-19ba4f8465d7",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.types import *\n",
    "#If we follow method1 (permissive with strict schema verification) or method2 (drop malformed with strict schema verification)\n",
    "#Challenges we have in this method1 and 2 is - unknown data loss at column level or at the row level\n",
    "#method3 best methodology of applying active data munging\n",
    "dfmethod3=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"PERMISSIVE\",header=False,inferSchema=True)\n",
    "#or\n",
    "print(dfmethod3.schema)\n",
    "strt11=StructType([StructField(\"id\",StringType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",StringType(),True),StructField(\"profession\",StringType(),True)])\n",
    "dfmethod3=spark.read.schema(strt11).csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"PERMISSIVE\",header=False)\n",
    "#Validation by doing cleansing (not at the time of creating Dataframe, rather we will clean and scrub subsequently)...\n",
    "display(dfmethod3)#We are going to validate the raw data without any scrubbing or cleansing applied, we will decide the cleansing/scrubbing strategy in the below stages...\n",
    "print(\"entire count of data\",dfmethod3.count())\n",
    "print(\"after cleansing, count of data\",len(dfmethod3.collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e884c46e-7628-4436-98de-7460c642c9a6",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Rejection Strategy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fbfccf0d-da18-48aa-8329-c3f15924c51a",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766816555750}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before actively Cleansing or Scrubbing - We have to create a Rejection Strategy to reduce data challenges in the future\n",
    "strt11=StructType([StructField(\"id\",IntegerType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",IntegerType(),True),StructField(\"profession\",StringType(),True),StructField(\"corruptdata\",StringType())])\n",
    "dfmethod3=spark.read.schema(strt11).csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"PERMISSIVE\",header=False,columnNameOfCorruptRecord=\"corruptdata\")\n",
    "display(dfmethod3)\n",
    "print(\"entire count of data\",dfmethod3.count())\n",
    "df_reject=dfmethod3.where(\"corruptdata is not null\")\n",
    "df_reject.drop(\"corruptdata\").write.mode(\"overwrite\").csv(\"/Volumes/workspace/wd36schema/ingestion_volume/rejects/\",mode=\"overwrite\")\n",
    "print(\"Data to reject or update the source\",len(dfmethod3.where(\"corruptdata is not null\").collect()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "5cf841cb-a560-4535-8fe1-480a11915ee4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Cleansing\n",
    "It is a process of cleaning/removing or making the data more clean Eg. Cutting/removing debris portion of the potato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "215273b4-03ac-4e9d-8d13-31e4503bafd4",
     "showTitle": false,
     "tableResultSettingsMap": {
      "3": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766818507325}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 3
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#We already know how to do cleansing applying the strict Structure on method1 and method2, but we can do cleansing and scrubbing in a controlled fashion by applying functions on the method3 dataframe\n",
    "#Important na functions we can use to do cleansing\n",
    "strt11=StructType([StructField(\"id\",StringType(),True),StructField(\"firstname\",StringType(),True),StructField(\"lastname\",StringType(),True),StructField(\"age\",StringType(),True),StructField(\"profession\",StringType(),True)])\n",
    "dfmethod3=spark.read.schema(strt11).csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",mode=\"PERMISSIVE\",header=False)\n",
    "display(dfmethod3.take(15))\n",
    "print(\"Actual DF count\",len(dfmethod3.collect()))\n",
    "cleansed_df1=dfmethod3.na.drop(how=\"any\")#drop the row, if any one column in our df row contains null\n",
    "cleansed_df1=dfmethod3.na.drop(how=\"any\",subset=[\"id\",\"age\"])#drop the row, if any one column id/age contains null\n",
    "print(\"cleansed any DF count\",len(cleansed_df1.collect()))\n",
    "display(cleansed_df1.take(15))\n",
    "cleansed_df2=dfmethod3.na.drop(how=\"all\")#drop the row, if all the columns in our df row contains null\n",
    "cleansed_df2=dfmethod3.na.drop(how=\"all\",subset=[\"id\",\"profession\"])#drop the row, if all the columns (id,profession) in our df row contains null\n",
    "print(\"cleansed all DF count\",len(cleansed_df2.collect()))\n",
    "display(cleansed_df2.take(15))\n",
    "\n",
    "#Threshold - least bother (if we need minimum this many number of columns with not nulls, then only we will keep the row)\n",
    "cleansed_df3=dfmethod3.na.drop(thresh=4,subset=[\"id\",\"firstname\",\"age\",\"profession\"])\n",
    "print(\"cleansed threshold DF count\",len(cleansed_df3.collect()))\n",
    "display(cleansed_df3.take(15))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "501db613-f37e-409b-b733-5803444e2bc4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before scrubbing, lets take the right cleansed data with id as null and entire row as null removed out\n",
    "#Finally I am arriving for our current data, lets perform the best cleansing\n",
    "cleansed_df=dfmethod3.na.drop(subset=[\"id\"]).na.drop(how=\"all\")\n",
    "print(\"Final cleansed DF\",len(cleansed_df.collect()))\n",
    "display(cleansed_df.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "b44cc3ee-eaab-4159-b2ac-2327336e69ea",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Scrubbing\n",
    "It is a process of polishing/reformatting data or making the data more tidy Eg. polishing/scrubbing the mud portion of the potato"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a8937f26-3c3e-401a-bdf5-367d5bada163",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766819109645}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Scrubbing will not remove the rows rather it will try to polish/scrub/reformat of the data in a usable format\n",
    "#Can be achieved using 2 functions under na (na.fill & na.replace)\n",
    "scrubbed_df1=cleansed_df.na.fill(\"na\",subset=[\"firstname\",\"lastname\"]).na.fill(\"not provided\",subset=[\"profession\"])\n",
    "scrubbed_df2=scrubbed_df1.na.replace(\"IT\",\"Information Technologies\",subset=[\"profession\"]).na.replace(\"Pilot\",\"Aircraft Pilot\",subset=[\"profession\"])\n",
    "#or\n",
    "#Find and Replace functionality\n",
    "dict1={\"IT\":\"Information Technologies\",\"Pilot\":\"Aircraft Pilot\",\"Actor\":\"Celebrity\"}\n",
    "scrubbed_df=scrubbed_df1.na.replace(dict1,subset=[\"profession\"])\n",
    "#scrubbed_df2=scrubbed_df1.na.replace(\"IT\",\"Information Technologies\",subset=[\"profession\"]).na.replace(\"Pilot\",\"Aircraft Pilot\",subset=[\"profession\"])\n",
    "print(\"scrubbed DF\",len(scrubbed_df.collect()))\n",
    "display(scrubbed_df.take(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cfd558ea-4a3e-427b-999c-891450d6de77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "####3. Standardization, De-Duplication and Replacement / Deletion of Data to make it in a usable format"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e156351e-474c-4880-9717-bf4ae17a22f2",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####Standardization - \n",
    "Making the data more standard by adding/removing/reordering columns as per the expected standard, unifying into expected format, converting the type as expected etc.,"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2a6febb4-3ca8-4a9f-8f17-651cff1f6aa4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization1 - Column Enrichment (Addition of columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "a3d82f7a-b464-4375-ad06-7d2826d8a1cc",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import lit,initcap\n",
    "#Standardization1 - Add columns (ENRICHMENT) to make this dataframe more standard for understanding of the source system\n",
    "standard_df1=scrubbed_df.withColumn(\"sourcesystem\",lit(\"Retail\"))#If we have to add a columns with some hardcoded value in dataframe, we have use lit function to add a hardcoded/literal value\n",
    "display(standard_df1.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "da7293f4-870f-4dd4-a7d1-4d707fac5d89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization2 - Column Uniformity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "77f93ada-7028-4d03-bbc3-aa7e70b0ccac",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Standardization2 - UNIFORMITY of the data\n",
    "display(standard_df1.groupBy(\"profession\").count())#DSL\n",
    "#standard_df1.createOrReplaceTempView(\"view1\")\n",
    "#display(spark.sql(\"select profession,count(1) from view1 group by profession\"))#Declarative lang\n",
    "standard_df2=standard_df1.withColumn(\"profession\",initcap(\"profession\"))#If we have to add a columns with some hardcoded value in dataframe, we have use lit function to add a hardcoded/literal value\n",
    "display(standard_df2.take(15))\n",
    "display(standard_df2.groupBy(\"profession\").count())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "904e1682-372a-4932-b5d3-a8ffdd3ff218",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization3 - Format Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "542b4370-af9d-444b-8749-ec929d03e4ab",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Standardization3 - Format Standardization\n",
    "from pyspark.sql.functions import *\n",
    "cid_standardization={\"one\":\"1\",\"two\":\"2\",\"ten\":\"10\"}#We can think of using GenAI here later\n",
    "standard_df3=standard_df2.na.replace(cid_standardization,subset=[\"id\"])#Using munging feature for standardizing the data\n",
    "standard_df3=standard_df3.withColumn(\"age\",regexp_replace(\"age\",\"-\",\"\"))\n",
    "display(standard_df3.take(15))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "3780a193-ea8d-4585-af19-3cad6bdccfeb",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization4 - Type Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "fc758063-bbc8-4b0a-93ab-39f4311bde06",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#I wanted to learn/test a function functionality\n",
    "#create a dummy dataframe, apply function to that dummy df to test your function's functionality\n",
    "spark.sql(\"select '123' as col1\").where(\"col1 not rlike '[a-zA-Z]'\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "19a36f12-ca3a-4c16-9fc3-7237bf2dac2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df3.printSchema()\n",
    "#display(len(standard_df3.collect()))\n",
    "#display(standard_df3.where(\"id like '%trailer%'\"))\n",
    "standard_df3=standard_df3.where(\"id not rlike '[a-zA-Z]'\")#Removed the string data in the id column\n",
    "#display(standard_df3.where(\"id='trailer_data:end of file'\"))\n",
    "#display(len(standard_df3.collect()))\n",
    "standard_df4=standard_df3.withColumn(\"age\",col(\"age\").cast(\"int\")).withColumn(\"id\",col(\"id\").cast(\"long\"))\n",
    "standard_df4.printSchema()\n",
    "display(standard_df4.take(15))\n",
    "#standard_df4.where(\"id=1000\").show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "461ec63e-c8a4-422d-8c1e-fb0baff8f802",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization5 - Naming Standardization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ad58903c-ca26-4883-a9aa-02e38a480c2f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df5=standard_df4.withColumnRenamed(\"id\",\"custid\")\n",
    "standard_df5.printSchema()\n",
    "display(standard_df5.take(15))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "90558fdc-ea11-482a-af85-a16ba8b4f59a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Standardization6 - Reorder Standadization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "2b541dca-eed0-4cc3-a1f3-c4be184aea43",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "standard_df6=standard_df5.select(\"custid\",\"age\",\"firstname\",\"lastname\",\"profession\",\"sourcesystem\")\n",
    "standard_df6.printSchema()\n",
    "display(standard_df6.take(15))\n",
    "#standard_df6.write.mode(\"overwrite\").saveAsTable(\"munged_cust_data\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "bbcdc7d1-4a34-48c1-8958-906288eb4439",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "Capture all the functions or data munging functions onwards used in this entire notebook"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1127de81-307f-46de-b55f-a440a7e254b5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####DeDuplication - De-Duplication and removal of non prioritized dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "c3515551-c415-4315-a8db-93319daf5bc2",
     "showTitle": false,
     "tableResultSettingsMap": {
      "1": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766900019806}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 1
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Duplicate Elimination at the record level, column level and in a priority basis of some column level higher age\n",
    "dedup_df1=standard_df6.where(\"custid in (4000001,4000003)\")\n",
    "dedup_df1=standard_df6.distinct()#Eliminating Record level duplicates\n",
    "dedup_df2=dedup_df1.dropDuplicates(subset=[\"custid\"])#Retains only the first row and eliminate the subsequent rows with duplicate keys without having any other priority at the age or any other columns\n",
    "#dedup_df1.where(\"custid in (4000001,4000003)\").orderBy([\"custid\",\"age\"],ascending=[True,False]).show()\n",
    "dedup_df2=dedup_df1.orderBy(subset=[\"custid\",\"age\"],ascending=[True,False]).coalesce(1)#Coalesing/combining all partition data into a single partition to get the dropduplicate output as expected\n",
    "display(dedup_df2)\n",
    "dedup_df3=dedup_df2.dropDuplicates([\"custid\"])#Retains only the first row and eliminate the subsequent rows with duplicate key\n",
    "display(dedup_df3)\n",
    "#dedup_df2.where(\"custid in (4000001,4000003)\").show()\n",
    "#How to cleanse the data with two same duplicate rows, except one of the column is different value?\n",
    "#we have 5 columns, only profession column is varying?\n",
    "#4000003\t41\tmohamed\tirfan\tInformation Technologies N\n",
    "#4000003 41\tmohamed\tirfan\tReporter Y\n",
    "dedup_df4=dedup_df1.dropDuplicates([\"custid\",\"age\",\"firstname\",\"lastname\"])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9c280deb-94bd-4cec-9f11-f234c321ae36",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##**2. Data Enrichment** - Detailing of data\n",
    "Makes your data rich and detailed <br>\n",
    "a. Add (withColumn,select,selectExpr), Derive (withColumn,select,selectExpr), Remove/Eliminate (drop,select,selectExpr), Rename (withColumnRenamed,select,selectExpr), Modify/replace (withColumn, select/selectExpr) - (very important spark sql functions) <br>\n",
    "b. split, merge/Concat <br>\n",
    "c. Type Casting, reformat & Schema Migration <br>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "05ae37bb-f160-4f09-a953-813aee225be1",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766900197501}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Before we enrich, lets do some EDA\n",
    "#Every stages we need to do basic EDA (Data Exploration)\n",
    "dedup_df3.printSchema()\n",
    "print(\"Records got cleaned/munged \",dfmethod1.count()-len(dedup_df3.collect()))\n",
    "display(dedup_df3.summary())\n",
    "display(dedup_df3)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "9589bfe8-46f2-4842-85f6-2ff5b7cbaf89",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####a. Add (withColumn,select,selectExpr), Derive (withColumn,select,selectExpr), Rename (withColumnRenamed,select,selectExpr), Modify/replace (withColumn, select/selectExpr), Remove/Eliminate (drop,select,selectExpr) - (very important spark sql functions)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "3bed3c28-1fe3-4131-86cd-51f7faa0ef94",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Adding of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "260a5589-bba2-4664-af8e-6672c6b1ccb3",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Adding columns\n",
    "datadt=\"25-12-25\"#source system data generated date\n",
    "enrich_df1=dedup_df3.withColumn(\"loaddt\",current_date()).withColumn(\"datadt\",lit(datadt))\n",
    "enrich_df1.show(2)\n",
    "enrich_df1=dedup_df3.withColumns({\"loaddt\":current_date(),\"datadt\":lit(datadt)})\n",
    "enrich_df1.show(2)\n",
    "#or\n",
    "enrich_df1=dedup_df3.withColumn(\"sourcesystem1\",col(\"sourcesystem\"))#Adding an existing column in a new column (copying a column value to a new column)\n",
    "enrich_df1.show(2)\n",
    "#or\n",
    "enrich_df1=dedup_df3.select(\"*\",current_date().alias(\"loaddt\"),lit(datadt).alias(\"datadt\"))#pure DSL\n",
    "enrich_df1.show(2)\n",
    "#or\n",
    "#colvar=f\"{datadt} as datadt\"\n",
    "datadt=\"25-12-25\"\n",
    "enrich_df1=dedup_df3.selectExpr(\"*\",\"current_date() as loaddt\",f\"'{datadt}' as datadt\")#Only select Function that supports both DSL and SQL\n",
    "enrich_df1.show(2)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "e984c43e-3f76-45f0-bf1d-eeca1c67e588",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Deriving of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7767295d-bf56-4590-8e6e-6e630ca39871",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrich_df2=enrich_df1.withColumn(\"profession_flag\",substring(\"profession\",1,1))\n",
    "enrich_df2.show(10)\n",
    "#or we can achieve using select\n",
    "enrich_df2=enrich_df1.select(\"*\",substring(\"profession\",1,1).alias(\"profession_flag\"))\n",
    "enrich_df2.show(10)\n",
    "#or we can achieve using selectExpr\n",
    "enrich_df2=enrich_df1.selectExpr(\"*\",\"substr(profession,1,1) as profession_flag\")\n",
    "enrich_df2.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "286ef171-2836-41c5-9d8d-7ce79d0cd278",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Renaming of columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "03076167-e4bb-4088-ad3c-1352061cc529",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrich_df3=enrich_df2.withColumnRenamed(\"profession_flag\",\"proflag\")#better to use\n",
    "enrich_df3.show(10)\n",
    "enrich_df3=enrich_df2.withColumnsRenamed({\"profession_flag\":\"proflag\",\"profession\":\"prof\"})\n",
    "enrich_df3.show(10)\n",
    "#or\n",
    "enrich_df3=enrich_df2.select(\"*\",col(\"profession\").alias(\"prof\"))#This will derive a new column called prof\n",
    "enrich_df3=enrich_df2.select(\"custid\",\"age\",\"firstname\",\"lastname\",col(\"profession\").alias(\"prof\"),\"sourcesystem\",\"loaddt\",\"datadt\",col(\"profession_flag\").alias(\"proflag\"))#This will derive a new column called prof#better to use\n",
    "#enrich_df3=enrich_df3.drop(\"profession\")#column orders are changed, not good to use\n",
    "enrich_df3.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "1c1b3ae9-5726-4550-8c24-f9bb9ecbb979",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Modify/replace (withColumn, select/selectExpr)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "e00edec7-2d17-46f8-8a18-c19291fd916f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrich_df3.printSchema()#datadt is not in a expected date format for further usage and i wanted to convert sourcesystem into uppercase\n",
    "enrich_df4=enrich_df3.withColumn(\"sourcesystem\",upper(col(\"sourcesystem\"))).withColumn(\"datadt\",to_date(col(\"datadt\"),'dd-MM-yy'))#This time withColumn didnt added a new column, it just modified the existing column\n",
    "enrich_df4.printSchema()#datadt is expected date format\n",
    "enrich_df4.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "bd6b4370-a9f6-4b07-b703-5ddc639f74e5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Remove/Eliminate (drop,select,selectExpr) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "dce618f2-34a8-464e-ac2f-944522c576dc",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766906802087}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "enrich_df5=enrich_df4.withColumn(\"fullname\",concat(col(\"firstname\"),lit(\" \"),col(\"lastname\")))\n",
    "enrich_df5=enrich_df5.drop(\"firstname\",\"lastname\").select(\"custid\",\"age\",\"fullname\",\"prof\",\"sourcesystem\",\"loaddt\",\"datadt\",\"proflag\")#drop function is good to use if we just want to drop a column in the df\n",
    "display(enrich_df5.limit(10))\n",
    "#or\n",
    "enrich_df5=enrich_df4.select(\"custid\",\"age\",concat(col(\"firstname\"),lit(\" \"),col(\"lastname\")).alias(\"fullname\"),\"prof\",\"sourcesystem\",\"loaddt\",\"datadt\",\"proflag\")#Better methodology\n",
    "display(enrich_df5.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "b283d6bb-68bd-4c27-9fe1-b14d9aa2fcf0",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######Conclusion/Best practices of using different column enrichment functions\n",
    "1. select is good to use if we want to perform - \n",
    "Good for ordering/reordering, only renaming column (not good), only reformatting/deriving a column (not good), **for all of these operation in a single iteration** such renaming, reordering, reformatting,deriving, dropping etc., (best to use)\n",
    "2. selectExpr is good to use if we want to perform - Same as select by using iso sql functionality (if we are not familiar in DSL) **for all of these operation in a single iteration**\n",
    "3. withColumn is good to use if we want to perform - \n",
    "**for adding/deriving/modifying/replacing in a single iteration**\n",
    "Adding/Deriving column(s) in the last (Good), Modifying/replacing (Good), Renaming (not good), Dropping(not possible)\n",
    "4. withColumnRenamed is good to use if we want to perform - only for renaming column (Good)\n",
    "5. drop is good to use if we want to perform - only dropping of columns in the given position (Good)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "220285d0-8353-4a44-9c3a-e07d82c8537d",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "#####b. Splitting & Merging/Melting of Columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "67ad126e-f567-4fe0-9c55-65496726774f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Splitting of columns\n",
    "enrich_df6=enrich_df5.withColumn(\"profsplit\",split(col(\"prof\"),' '))\n",
    "enrich_df6=enrich_df6.withColumns({\"proffirst\":col(\"profsplit\")[0],\"proflast\":col(\"profsplit\")[size(col(\"profsplit\"))-1]})\n",
    "display(enrich_df6.limit(10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "37eaeaae-ea1d-416f-8e93-bb23eca1f10c",
     "showTitle": false,
     "tableResultSettingsMap": {
      "0": {
       "dataGridStateBlob": "{\"version\":1,\"tableState\":{\"columnPinning\":{\"left\":[\"#row_number#\"],\"right\":[]},\"columnSizing\":{},\"columnVisibility\":{}},\"settings\":{\"columns\":{}},\"syncTimestamp\":1766909271349}",
       "filterBlob": null,
       "queryPlanFiltersBlob": null,
       "tableResultIndex": 0
      }
     },
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Merging of columns\n",
    "enrich_df7=enrich_df6.withColumn(\"proflag\",concat(substring(col(\"proffirst\"),1,1),substring(col(\"proflast\"),1,1))).drop(\"profsplit\")\n",
    "display(enrich_df7)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "f91b011f-565e-4e91-98e7-f66fb5b8774a",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "######c. Typecasting & Formatting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "25dcbd54-2aed-493c-a898-04f87266b643",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Consider the below casting and formatting\n",
    "enrich_df7.printSchema()#datadt is not in a expected date format for further usage and i wanted to convert sourcesystem into uppercase\n",
    "#enrich_df4=enrich_df3.withColumn(\"sourcesystem\",upper(col(\"sourcesystem\"))).withColumn(\"datadt\",to_date(col(\"datadt\"),'dd-MM-yy'))#This time withColumn didnt added a new column, it just modified the existing column\n",
    "#enrich_df4.printSchema()#datadt is expected date format\n",
    "#enrich_df4.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "a5a13909-ec09-4e2b-ae67-3e3b2c37db5f",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "##3. Data Customization & Processing - Application of Tailored Business specific Rules a. User Defined Functions b. Building of Frameworks & Reusable Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "16ec4630-106b-4ae7-b1cf-ea3f81435fe4",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "from pyspark.sql.functions import udf\n",
    "#1. Create a Python function\n",
    "def convert_to_upper(name):\n",
    "    return name.upper()+'-Inceptez'\n",
    "#2. Register the python as dataframe supporting user defined function\n",
    "user_def_fun=udf(convert_to_upper)\n",
    "#3. Apply the udf in my dataframe columns\n",
    "enrich_df8=enrich_df7.withColumn(\"fullname\",user_def_fun(col(\"fullname\")))\n",
    "display(enrich_df8.limit(10))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "62bbe034-7274-45e6-ae89-9f83243aeea9",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": [
    "1. how to create pipelines using different data processing techniques by connecting with different sources/targets\n",
    "2. how to Standardize/Modernization/Industrializing the code and how create/consume generic/reusable functions & frameworks\n",
    "3. Testing (Unit, Peer Review, SIT/Integration, Regression, User Acceptance Testing), Masking engine,\n",
    "4. Reusable transformation(munge_data, optimize_performance),\n",
    "5. Quality suite/Data Profiling/Audit engine (Reconcilation) (Audit framework), Data/process Observability\n",
    "\n",
    "6. how terminologies/architecture/submit jobs/monitor/log analysis/packaging and deployment ...\n",
    "7. performance tuning\n",
    "8. Deploying spark applications in Cloud & other Distributions like Hortonworks/Cloudera/Databricks\n",
    "9. Creating cloud pipelines using spark SQL programs & Cloud native tools\n",
    "\n",
    "What is the importance of learning this program or How this can address interview questions..?\n",
    "VERY VERY IMPORTANT PROGRAM IN TERMS OF EXPLAINING/SOLVING PROBLEMS GIVEN IN INTERVIEW ,\n",
    "WITH THIS ONE PROGRAM YOU CAN COVER ALMOST ALL DATAENGINEERING FEATURES\n",
    "Tell me about the common transformations you performed,\n",
    "tell me your daily roles in DE,\n",
    "tell me some business logics you have writtened recently\n",
    "How do you write an entire spark application,\n",
    "levels/stages of DE pipelines or\n",
    "have you created DE pipelines what are the transformations applied,\n",
    "how many you have created or are you using existing framework or you created some framework?\n",
    "\n",
    "'''\n",
    "TRANSFORMATION & ANALYTICAL TECHNIQUES\n",
    "Starting point - (Data Governance (security) - Tagging, categorization, classification, masking/filteration)\n",
    "1. Data Munging - Process of transforming and mapping data from Raw form into Tidy(usable) format with the\n",
    "intent of making it more appropriate and valuable for a variety of downstream purposes such for\n",
    "further Transformation/Enrichment, Egress/Outbound, analytics, model application & Reporting\n",
    "a. Passive - Data Discovery EDA (Exploratory Data Analytics)\n",
    "(every layers ingestion/transformation/analytics/consumption) -\n",
    "Performing an (Data Exploration) exploratory data analysis of the raw data to identify the attributes and patterns.\n",
    "b. Active - Combining Data + Schema Evolution/Merging (Structuring)\n",
    "c. Validation, Cleansing, Scrubbing - Identifying and filling gaps & Cleaning data to remove outliers and inaccuracies\n",
    "Preprocessing, Preparation\n",
    "Cleansing (removal of unwanted datasets eg. na.drop),\n",
    "Scrubbing (convert of raw to tidy na.fill or na.replace),\n",
    "d. Standardization, De Duplication and Replacement & Deletion of Data to make it in a usable format (Dataengineers/consumers)\n",
    "\n",
    "2. Data Enrichment - Makes your data rich and detailed\n",
    "a. Add, Remove, Rename, Modify/replace\n",
    "b. split, merge/Concat\n",
    "c. Type Casting, format & Schema Migration\n",
    "\n",
    "3. Data Customization & Processing - Application of Tailored Business specific Rules\n",
    "a. User Defined Functions\n",
    "b. Building of Frameworks & Reusable Functions\n",
    "\n",
    "4. Data Curation\n",
    "a. Curation/Transformation\n",
    "b. Analysis/Analytics & Summarization -> filter, transformation, Grouping, Aggregation/Summarization\n",
    "\n",
    "5. Data Wrangling - Gathering, Enriching and Transfomation of pre processed data into usable data\n",
    "a. Lookup/Reference\n",
    "b. Enrichment\n",
    "c. Joins\n",
    "d. Sorting\n",
    "e. Windowing, Statistical & Analytical processing\n",
    "f. Set Operation\n",
    "\n",
    "6. Data Publishing & Consumption - Enablement of the Cleansed, transformed and analysed data as a Data Product.\n",
    "a. Discovery,\n",
    "b. Outbound/Egress,\n",
    "c. Reports/exports\n",
    "d. Schema migration\n",
    "'''"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "01d9b849-607e-4852-9687-b41332caa867",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "source": []
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 4
   },
   "notebookName": "BB_End_End_Transformations",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
