{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "17039d3b-9f2c-4f14-a498-f06de54b00ce",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "#Trade off between DF & Temp view or DSL & SQL\n",
    "#1. By default spark produces DF(mostly)\n",
    "#2. DF(XLS) -> DSL functions\n",
    "#3. DF(XLS) to Temp View(Temptable) -> SQL Queries\n",
    "#4. DSL/SQL internally RDD transformations & actions\n",
    "\n",
    "#We have to understand, how to represent DF to View & View to DF (natural), DF to DF (natural), View to View"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cf5f9c0f-0683-4646-be48-fc6031b255cf",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "rawdf1=spark.read.csv(\"/Volumes/we47catalog/we47schema/we47_volume/custsmodified\",header=False,inferSchema=True).toDF(\"id\",\"firstname\",\"lastname\",\"age\",\"profession\")#DF\n",
    "print(rawdf1)\n",
    "df2=rawdf1.select(\"*\").where(\"age>'60'\")#DSL\n",
    "df2.show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "ac9e735f-3945-4415-aaf7-0e55f7f51255",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def fun1(colinput):\n",
    "    return colinput.upper()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "9b3d37c2-ec62-4715-a7b0-ba4cd89594a5",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#from pyspark.sql.functions import udf\n",
    "#udffun1=udf(fun1)#works for dsl columns, but will not work for sql columns\n",
    "spark.udf.register(\"udfsqlfun1\",fun1)#works for sql columns\n",
    "rawdf1.createOrReplaceTempView(\"rawdftv\")\n",
    "df2=spark.sql(\"select *,udfsqlfun1(firstname) from rawdftv where age>'60'\")\n",
    "df2.select(\"*\").show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "06e12092-9bd1-4d1b-9117-f442c1beae02",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "%python\n",
    "#For extraction/load operations with multiple features, preferably use DSL and not SQL\n",
    "#For initial dataset creation or for ingestion/egress DSL way of creating dataframe is better, because not all the options are supported in SQL ingestion\n",
    "spark.sql(\"\"\"CREATE OR REPLACE TEMPORARY VIEW rawdf1view\n",
    "(\n",
    "  id INT,\n",
    "  firstname STRING,\n",
    "  lastname STRING,\n",
    "  age INT,\n",
    "  profession STRING\n",
    ")\n",
    "USING CSV\n",
    "OPTIONS (\n",
    "  path \"/Volumes/workspace/default/volume1/custs\",\n",
    "  header \"false\",\n",
    "  inferSchema \"false\" \n",
    ");\"\"\")\n",
    "\n",
    "spark.sql(\"select * from rawdf1view limit 20\").show(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {},
     "inputWidgets": {},
     "nuid": "59ef9e45-6bd0-406a-9165-927bfd672a77",
     "showTitle": false,
     "tableResultSettingsMap": {},
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "original_filename='custsmodified_25/30/12.csv'\n",
    "derived_datadt=original_filename.split('_')[1].split('.')[0]\n",
    "spark.sql(f\"\"\"\n",
    "    create or replace temp view enrichdffinal as \n",
    "    SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    concat_ws(' ', firstname, lastname) AS fullname,\n",
    "    split(concat(profession, '-', substring(profession, 1, 1)), '-')[0] AS profession,\n",
    "    srcsystem AS sourcename,\n",
    "    --substring(profession, 1, 1) AS profflag,\n",
    "        to_date('{derived_datadt}', 'yy/dd/MM') AS datadt,\n",
    "        current_date() AS loaddt,\n",
    "        upper(substring(profession, 1, 3)) AS shortprof\n",
    "    FROM mungeddf\n",
    "\"\"\")\n",
    "spark.sql(\"select * from enrichdffinal\").show()\n",
    "custid,\n",
    "    age,\n",
    "    fullname,\n",
    "    profession,\n",
    "    sourcename,\n",
    "    to_date(datadt, 'yy/dd/MM') AS datadt, -- Converts string '25/30/12' to Date '2025-12-30'\n",
    "    loaddt,\n",
    "    shortprof\n",
    "\n",
    "\n",
    "create or replace temp view enrichdf5 as \n",
    "SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    firstname,\n",
    "    lastname,\n",
    "    profession,\n",
    "    sourcename,\n",
    "    datadt,\n",
    "    loaddt\n",
    "FROM enrichdf4;\n",
    "\n",
    "create or replace temp view mergeddf as \n",
    "SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    concat_ws(' ', firstname, lastname) AS fullname, -- Merging firstname and lastname\n",
    "    split(profession, '-')[0] AS profession, -- Splitting and taking the first element\n",
    "    sourcename,\n",
    "    datadt,\n",
    "    loaddt,\n",
    "    upper(substring(split(profession, '-')[0], 1, 3)) AS shortprof -- creating shortprof based on the split result\n",
    "FROM enrichdf5;\n",
    "\n",
    "create or replace temp view formatteddf as \n",
    "SELECT \n",
    "    custid,\n",
    "    age,\n",
    "    fullname,\n",
    "    profession,\n",
    "    sourcename,\n",
    "    to_date(datadt, 'yy/dd/MM') AS datadt, -- Converts string '25/30/12' to Date '2025-12-30'\n",
    "    loaddt,\n",
    "    shortprof\n",
    "FROM mergeddf;\n",
    "SELECT * FROM formatteddf LIMIT 10;"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "computePreferences": null,
   "dashboards": [],
   "environmentMetadata": {
    "base_environment": "",
    "environment_version": "4"
   },
   "inputWidgetPreferences": null,
   "language": "python",
   "notebookMetadata": {
    "mostRecentlyExecutedCommandWithImplicitDF": {
     "commandId": 8935851003814796,
     "dataframes": [
      "_sqldf"
     ]
    },
    "pythonIndentUnit": 4
   },
   "notebookName": "BB2_Generic_Framework_Function_Based_DSL_SQL",
   "widgets": {}
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
